Introduction
------------
The watchdog module is a 'loosely supervisorlike' behaviour designed to keep 
processes which it supervises up and running. It differs from a normal 
supervisor in that it applies a simple backoff mechanism when restarting
its watched processes.

watchdog does not replace simple_one_for_one supervisors, but provides a 
slightly different view to managing processes uptime.

Contraints
----------
    - Should very simple and _almost_ non-invasive to the watched processes
    - Should feel very much like a simple_one_for_one supervisor except 
      when not.
    - Should fit seamlessly into a supervision tree.
    - Should concentrate only on keeping the children alive, restarting
      them and providing helper apis to help with managing that.
    - Should feel familiar to use


Overview
--------
Watchdog is really a gen_server wrapping up access to a simple_one_for_one
supervisor. While providing the restart services itself.

The child spec is added to the simple_one_for_one supervisor, with the childs
restart type set to 'temporary' by the gen_server. This is done so that
the supervisor itself does not attempt to restart the child. 

The watchdog gen_server calls supervisor:start_child to start the child
processes when the need arises.

Monitoring
----------
When the watchdog gen_server starts a child successfully via 
supervisor:start_child, it calls erlang:monitor/2 on the child Pid. 

With this setup, the watchdog will receive a 'DOWN' message when the 
child goes down and is able to initiate (uber secret!!) restart measures


Initialization
--------------
During initialization, the module implementing the watchdog behaviour needs 
to supply the following:
    - A ChildSpec as expected by a supervisor in the [ChildSpec] list
    - The total number children to restart. Watchdog will always strive to 
      ensure this number of children are running. It will continually retry
      starting dead children untill all are up and running.
    - The watchdog restart spec which looks like {Min, Max, Delta}:
        - Min is the smallest interval b/w restarts. When a child dies, 
          watchdog will initially schedule it for restarting in Min 
          milliseconds
        - Max is the largest interval b/w consecutive restarts. When a child
          dies and is restarted, if that restart is not successfull, watchdog
          will backoff a bit then attempt the restart again. It will keep doing
          this until the child starts successfully or the backoff interval
          reaches Max. At which point, it won't increase the backoff interval
          anymore.
        - Delta is the backoff delta b/w failed restarts.

The watchdog rewrites the ChildSpec and sets the restart type to 'temporary',
then passes the spec to the internal simple_one_for_one and starts the 
supervisor, keeping a handle on the supervisor Pid in the watchdog state.

After the internal supervisor is started up, the watchdog gen_server sets a
timeout for 500 milliseconds and is considered ready and operational.

When the timeout occurs, the gen_server will attempt to start the specified 
total number of children. 


Child Ids
---------
Just like a normal supervisor (except the simple_one_for_one), the watchdog
has a concept of child Ids. This is used to uniquely identify the child 
instance regardless of its current Pid or even if its currently running or 
is waiting to be restarted.

This child Ids can be used to access various information on the child 
processes and to control the child processes without knowing their Pid.

To keep it simple, since each child is started from the same child spec,
the watchdog uses a one-based positional index to identify each child. 

The index corresponds to the order in which the child processes are started, 
so if there are a total of 3 child processes, the first one has an id of 1,
the second an  id of 2, etc


Uptime Characteristics
----------------------
One of the nicer features provided by the watchdog is runtime characteristics
of the watched child processes. The watchdog maintains the following 
information for each child Id in an ETS table which can be queried:
    - Id: The child id
    - StartTime: The timestamp of the last time the child was successfully 
      started
    - Startups: How many times the child has been successfully started by the
      watchdog.
    - Total Uptime: The cummulative total uptimes of the child process. Each
      uptime is a measure of the time between the last StartTime and the
      time the watchdog recieved a 'DOWN' message for that child id

Watchdog then keeps a Pid:Id map in its state that relates a ChildId to its
current running Pid (if the child is currently running ofcourse).

These characteristics are reported by watchdog:which_children/1, and
watchdog:child_info/2


